{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\anike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\anike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\anike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\anike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     C:\\Users\\anike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import feedparser as fp\n",
    "import json\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "from time import mktime\n",
    "from datetime import datetime\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import nltk\n",
    "import re\n",
    "from rouge import Rouge \n",
    "from collections import Counter\n",
    "from geotext import GeoText\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.downloader.download('maxent_ne_chunker')\n",
    "nltk.downloader.download('words')\n",
    "nltk.downloader.download('treebank')\n",
    "nltk.downloader.download('maxent_treebank_pos_tagger')\n",
    "import string\n",
    "from gensim.summarization.summarizer import summarize\n",
    "\n",
    "from sumy.parsers.plaintext import PlaintextParser #We're choosing a plaintext parser\n",
    "from sumy.nlp.tokenizers import Tokenizer \n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer #We're choosing Lexrank, other algorithms are also built in\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "\n",
    "import articleDateExtractor\n",
    "\n",
    "\n",
    "global str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "# nltk.internals.config_java(\"C:/Program Files/Java/jdk1.8.0_181/bin\")\n",
    "\n",
    "import os\n",
    "java_path = \"C:/Program Files/Java/jdk1.8.0_181/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "st = StanfordNERTagger('C:/Users/anike/Downloads/stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.crf.ser.gz','C:/Users/anike/Downloads/stanford-ner-2018-10-16/stanford-ner-3.9.2.jar', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anike\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "C:\\Users\\anike\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\anike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas as pd, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "import spacy\n",
    "import re\n",
    "import csv\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from  sklearn.metrics  import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = nltk.WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess(rawText):\n",
    "    cleanText = re.sub(r'[^\\w\\s]','',rawText.lower())\n",
    "    tokens = word_tokenize(cleanText)\n",
    "    tokens = [token for token in tokens if not token in stop_words]\n",
    "    tokens = [lemma.lemmatize(token) for token in tokens ]\n",
    "    processedText = (\" \".join(tokens))\n",
    "    return processedText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(r'C:\\Users\\anike\\indonesiaEventCorpus_2010_2019.csv')\n",
    "# data = open(r'C:\\Users\\anike\\indiaCorpus_2010_2019.csv')\n",
    "\n",
    "target, notes = [], []\n",
    "reader = csv.reader(data, delimiter=\",\")\n",
    "next(reader, None)\n",
    "for i, line in enumerate(reader):\n",
    "    target.append(line[1])\n",
    "    rawText = line[0]\n",
    "    notes.append(preprocess(rawText))\n",
    "\n",
    "dataSet = pd.DataFrame()\n",
    "dataSet['notes'] = notes\n",
    "dataSet['target'] = target\n",
    "dataSet = dataSet.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(dataSet['notes'], dataSet['target'])\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "vector = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "vector.fit(dataSet['notes'])\n",
    "train_x =  vector.transform(train_x)\n",
    "valid_x =  vector.transform(valid_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naieve Bayes Event Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = MultinomialNB().fit(train_x, train_y)\n",
    "predicted = clf.predict(valid_x)\n",
    "print(accuracy_score(valid_y,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Event Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8514285714285714"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "SVM = svm.LinearSVC()\n",
    "SVM.fit(train_x, train_y)\n",
    "predict = SVM.predict(valid_x)\n",
    "accuracy_score(valid_y,predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Event Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7771428571428571"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=10)\n",
    "rf.fit(train_x, train_y)\n",
    "predict = rf.predict(valid_x)\n",
    "accuracy_score(valid_y,predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 4\n",
    "data = {}\n",
    "data['newspapers'] = {}\n",
    "articlesData ={}\n",
    "collections = {\n",
    "    'total' : []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'World News Report': {'rss': 'https://www.einnews.com/rss/T2tHVmwcBIBT0pom',\n",
       "  'link': 'https://www.einnews.com/rss/T2tHVmwcBIBT0pom'}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open('newspaperLinks.json') as file:\n",
    "with open('newspaperLinks_Indonesia.json') as file:\n",
    "    sources = json.load(file)\n",
    "sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizeText(text):\n",
    "    string=text\n",
    "    parser = PlaintextParser.from_string(string, Tokenizer(\"english\"))\n",
    "\n",
    "    summarizer = LexRankSummarizer()\n",
    "    summary_lex = summarizer(parser.document, 1) #Summarize the document with 5 sentences\n",
    "    summary_1=''\n",
    "    for line in summary_lex:\n",
    "        #summary_1+=line\n",
    "        #print(line)\n",
    "        summary_1+=str(line)\n",
    "#     print(summary_1)\n",
    "\n",
    "#     lsa_summarizer = LsaSummarizer()\n",
    "#     summary_lsa =lsa_summarizer(parser.document,3)\n",
    "#     summary_2=''\n",
    "#     print(\"\\n\")\n",
    "#     for line in summary_lsa:\n",
    "#         summary_2+=str(line)\n",
    "#     print(summary_2)\n",
    "\n",
    "#     luhn_summarizer = LuhnSummarizer() \n",
    "#     luhn_summarizer.stop_words = (\"I\",  \"me\", \"is\", \"am\", \"the\",\"than\", \"that\", \"this\",\"an\",\"you\", \"are\")\n",
    "#     summary_3=''\n",
    "#     print(\"\\n\")\n",
    "#     for line in luhn_summarizer(parser.document, 3):\n",
    "#         summary_3+=str(line)\n",
    "#     print(summary_3)\n",
    "    return summary_1\n",
    "\n",
    "def getLocation(ents, text):\n",
    "    loc = \"\"\n",
    "#     locList = []\n",
    "    items = [x.text for x in ents if x.label_ in 'GPE' or x.label_ in 'LOC']\n",
    "    loc_ = [Counter(items).most_common(1)]\n",
    "    places = GeoText(text)\n",
    "    topPlace = [Counter(places.cities).most_common(1)]\n",
    "    \n",
    "    places = (places.cities)\n",
    "#     text = re.sub(r'[^\\w\\s]','',text)\n",
    "#     tokenized_text = word_tokenize(text)\n",
    "#     classified_text = st.tag(tokenized_text)\n",
    "#     stanfordLocations =[]\n",
    "#     for x in classified_text:\n",
    "#         if(x[1]==\"LOCATION\"):\n",
    "#             stanfordLocations.append(x[0])\n",
    "#     topPlace = [Counter(stanfordLocations).most_common(1)]\n",
    "#     locList = [i for i in stanfordLocations if i in items]\n",
    "    locList = [i for i in places if i in items]\n",
    "    total = items+locList \n",
    "    location_ = [Counter(total).most_common(1)]\n",
    "    if len(location_) == 0:\n",
    "        if(len(loc_))!=0:\n",
    "            loc = loc_[0][0][0]\n",
    "        elif (len(topPlace)!=0):\n",
    "            loc = topPlace[0]\n",
    "        else:\n",
    "            loc = \"\"\n",
    "    else:\n",
    "        if(len(location_))!=0:\n",
    "            if(len(location_[0]))!=0:\n",
    "                if(len(location_[0][0]))!=0:\n",
    "                    loc = location_[0][0][0]\n",
    "        else:\n",
    "            loc = \"unknown\"\n",
    "    return loc\n",
    "\n",
    "def getEventType(tokens):\n",
    "#     eventTypes ={\n",
    "#     'civilianViolence':['rebel','violence','militia','war','rape','torture','kill','shoot','fight','murder','molest','attack','terror','battle','assault','blood'],\n",
    "#     'riotsProtest':['protest','riot']\n",
    "#     }\n",
    "    eventType = \"other\"\n",
    "    for event, types in eventTypes.items():\n",
    "        tag = [token for token in tokens if any(typ in token for typ in types)]\n",
    "        if len(tag) != 0:\n",
    "            eventType = event\n",
    "            break\n",
    "    return eventType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Scraper and Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World News Report\n",
      "The Natural Resource Oligarchy Funding Indonesia’s Election\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anike\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Other']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anike\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting date from https://www.einnews.com/article/484901884/87tTuc0PYCYyE6Df?ref=rss&ecode=T2tHVmwcBIBT0pom\n",
      "Exception in extractArticlePublishedDate for https://www.einnews.com/article/484901884/87tTuc0PYCYyE6Df?ref=rss&ecode=T2tHVmwcBIBT0pom\n",
      "()\n",
      "'NoneType' object has no attribute 'strftime'\n",
      "Friday's best photos: Indonesia protests and VR headsets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anike\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Protests']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anike\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting date from https://www.einnews.com/article/484834140/67AW7Wz6tx7nKhR3?ref=rss&ecode=T2tHVmwcBIBT0pom\n",
      "Indonesia election tainted, opposition marchers say, but government warns against treason - World\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anike\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Protests']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anike\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting date from https://www.einnews.com/article/484820064/6yCY5qg2viNG62u5?ref=rss&ecode=T2tHVmwcBIBT0pom\n",
      "Australia’s voters are poised to punish the government\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anike\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Protests']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anike\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting date from https://www.einnews.com/article/484811977/cNa5YttM44kS5tnN?ref=rss&ecode=T2tHVmwcBIBT0pom\n",
      "ICMI calls on govt to help losing camp overcome disappointment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anike\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Protests']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anike\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting date from https://www.einnews.com/article/484791521/N3MVFTlAegD5e9D_?ref=rss&ecode=T2tHVmwcBIBT0pom\n",
      "Article `download()` failed with 404 Client Error: Not Found for url: http://www.newsonjapan.com/2019/05/10/lessons-learnt-from-indonesian-and-malaysian-approaches-to-terrorist-rehabilitation/ on URL https://www.einnews.com/article/484778218/xOcb2vKEQC_0l2B_?ref=rss&ecode=T2tHVmwcBIBT0pom\n"
     ]
    }
   ],
   "source": [
    "collections = {\n",
    "    'total' : []\n",
    "}\n",
    "from time import mktime\n",
    "from datetime import datetime\n",
    "from lexrank import STOPWORDS, LexRank\n",
    "\n",
    "event_type = \"\"\n",
    "data = {}\n",
    "data['newspapers'] = {}\n",
    "articlesData ={}\n",
    "collectedNews = {}\n",
    "politics= [\"politicize\", \"political affairs\",\"election\",\"polling\", \"administration\", \"Indonesia Politics\", \"politicians\", \"elections\", \"votes\", \"voters\",\"vote\",\"voting\"]\n",
    "violence = [\"violence\", \"criminal violence\", \"criminal\", \"crime\", \"criminal party\", \"election violence\", \"electoral violence\",\"mob killing\", \"vigilante\", \"riots\", \"killing\" ]\n",
    "parties = ['democratic Party', 'golkar', 'indonesian democratic party of struggle ', 'pdpi', 'pdp-i', 'prosperous justice party', 'pks', 'national mandate party', 'pan', 'united development party', 'ppp', 'national awakening party', 'pkb','great indonesia movement party','gerindra','peoples conscience party','hanura','nasdem party','indonesian solidarity party','psi','berkarya Party']\n",
    "events = ['arrest','arrested','violence against civilians','rioting','protesting','riots','protest','battles','violence','remote violence','assault','attack','bloodshed','brutality','clash','confusion','cruelty','disorder','disturbance','fighting','rampage','struggle','terrorism','abandon','acuteness','bestiality','blowup','coercion','compulsion','constraint','destructiveness','duress','ferocity','fervor','fierceness','flap','frenzy','fury','fuss','harshness','murderousness','onslaught','passion','power','roughness','ruckus','rumble','savagery','scam','severity','sharpness','storm','storminess','tumult','turbulence','uproar','vehemence','wildness','brute','force','foul','play','raging','anarchy','brawl','disturbance','lawlessness','protest','storm','strife','trouble','turbulence','turmoil','uproar','anarchism','burst','commotion','confusion','distemper','flap','fray','free-for-all','fuss','hassle','misrule','mix-up','quarrel','racket','row','ruckus','ruction','rumble','rumpus','run-in','scene','shivaree','shower','snarl','stir','to-do','tumult','brannigan','mob','violence','street','fighting','wingding','action','assault','attack','bloodshed','bombing','campaign','clash','combat','conflict','crusade','encounter','fighting','hostility','skirmish','strife','struggle','war','warfare','barrage','brush','carnage','contention','engagement','fray','havoc','onset','onslaught','press','ravage','scrimmage','sortie','blitzkreig']\n",
    "\n",
    "eventTypes ={\n",
    "'riots':['riot'],\n",
    "'protests':['protest','march'], \n",
    "'civilianViolence':['rebel','violence','militia','war','rape','torture','kill','shoot','fight','murder','molest','attack','terror','battle','assault','blood']\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "count = 1\n",
    "\n",
    "for source, link in sources.items():\n",
    "    print(source)\n",
    "    count = 1\n",
    "    if 'rss' in link:\n",
    "        parsedFeed = fp.parse(link['rss'])\n",
    "        collected = {\n",
    "            'rss': link['rss'],\n",
    "            'link': link['link'],\n",
    "            'article': []\n",
    "        }\n",
    "        for feed in parsedFeed.entries:\n",
    "            \n",
    "            try:\n",
    "\n",
    "                articlesData ={}\n",
    "                articles = Article(feed['links'][0]['href'])\n",
    "                articles.download()\n",
    "                articles.parse()\n",
    "                articles.nlp()\n",
    "                strg = re.sub(r'[^\\w\\s]','',articles.text)\n",
    "                articleTokens = nlp(strg)\n",
    "\n",
    "                tokens =[]\n",
    "                for token in articleTokens:\n",
    "                    tokens.append(token.text.lower())\n",
    "\n",
    "                txt = str(articles.text).split()\n",
    "\n",
    "                keyWords = articles.keywords\n",
    "\n",
    "                eventType = \"other\"\n",
    "                if(([a for a in keyWords if any(b.lower() in a.lower() for b in politics)])):\n",
    "                    print(articles.title)\n",
    "                    articlesData['title'] = articles.title\n",
    "\n",
    "                    article = nlp(articles.text)\n",
    "                    ###Summary\n",
    "                    sentences = [x for x in article.sents]\n",
    "                    sentences = list( map(str, sentences) )\n",
    "                    lxr = LexRank(articles.text)\n",
    "                    lxr.get_summary(sentences)\n",
    "                    summary = lxr.get_summary(sentences, summary_size=1, threshold=.1)\n",
    "                    summary = (\" \".join(summary))\n",
    "                    \n",
    "                    rawSummary = [preprocess(summary)]\n",
    "                    textVector = vector.transform(rawSummary)\n",
    "                    prediction = rf.predict(textVector)\n",
    "                    print(encoder.inverse_transform(prediction))\n",
    "                    articlesData['event'] = encoder.inverse_transform(prediction[0])\n",
    "\n",
    "                    dates = articleDateExtractor.extractArticlePublishedDate(feed['links'][0]['href'])\n",
    "                    localFormat = dates.strftime(\"%m/%d/%Y\")\n",
    "                    summaryDate = dates.strftime(\"On %d %b,\")\n",
    "                    articlesData['eventDate'] = localFormat\n",
    "                    articlesData['location'] = getLocation(articleTokens.ents, articles.text)\n",
    "                    orgs = set(keyWords).intersection(set(parties))\n",
    "\n",
    "                    if len(orgs) == 0:\n",
    "                        if(articlesData['event']==\"Riots\"):\n",
    "                            taggedOrg = \"Rioters\"\n",
    "                        elif(articlesData['event']==\"Protests\"):\n",
    "                            taggedOrg = \"Protestors\"\n",
    "                        else:\n",
    "                            taggedOrg = \"other\"\n",
    "                    else:\n",
    "                        taggedOrg=\"\"\n",
    "                        for org in orgs:\n",
    "                            taggedOrg+=org+\", \"\n",
    "                        taggedOrg=taggedOrg[:-2]\n",
    "\n",
    "                    articlesData['partiesInvolved'] = taggedOrg\n",
    "                    articlesData['source'] = source\n",
    "                    totalSummary = []\n",
    "                    totalSummary.append(summaryDate)\n",
    "                    totalSummary.append(summary)\n",
    "                    extractiveSummary  = (\" \".join(totalSummary))\n",
    "                    articlesData['summary'] = extractiveSummary\n",
    "                    articlesData['link'] = (feed['links'][0]['href'])\n",
    "                    collected['article'].append(articlesData)\n",
    "#                 else:\n",
    "#                     break\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "        collections['total'].append(collected)\n",
    "    else:\n",
    "        parsedFeed = newspaper.build(link['link'], memoize_articles = False)\n",
    "        collected = {\n",
    "            'link': link['link'],\n",
    "            'article': []\n",
    "        }\n",
    "        for feed in parsedFeed.articles:\n",
    "#             if count > 1000:\n",
    "#                 break\n",
    "            try:\n",
    "                feed.download()\n",
    "                feed.parse()\n",
    "                feed.nlp()\n",
    "                if feed.publish_date is not None:\n",
    "                    articlesData ={}\n",
    "                    \n",
    "                    strg = re.sub(r'[^\\w\\s]','',feed.text)\n",
    "                    articleTokens = nlp(strg)\n",
    "                    tokens =[]\n",
    "                    for token in articleTokens:\n",
    "                        tokens.append(token.text.lower())\n",
    "\n",
    "                        txt = str(feed.text).split()\n",
    "                        for i in txt:\n",
    "                            for j in events:\n",
    "                                if i==j:\n",
    "                                    event_type = j\n",
    "                                    break\n",
    "                                else:\n",
    "                                    event_type = \"other\"\n",
    "                    keyWords = feed.keywords\n",
    "                    if(([a for a in keyWords if any(b.lower() in a.lower() for b in politics)])):\n",
    "                        \n",
    "                        articlesData['title'] = feed.title\n",
    "                        article = nlp(feed.text)\n",
    "                        ###Summary\n",
    "                        sentences = [x for x in article.sents]\n",
    "                        sentences = list( map(str, sentences) )\n",
    "                        lxr = LexRank(feed.text)\n",
    "                        lxr.get_summary(sentences)\n",
    "                        summary = lxr.get_summary(sentences, summary_size=1, threshold=.1)\n",
    "                        summary = (\" \".join(summary))\n",
    "\n",
    "                        rawSummary = [preprocess(summary)]\n",
    "                        textVector = vector.transform(rawSummary)\n",
    "                        prediction = rf.predict(textVector)\n",
    "                        print(encoder.inverse_transform(prediction))\n",
    "                        articlesData['event'] = encoder.inverse_transform(prediction[0])\n",
    "                        \n",
    "#                         articlesData['event'] = getEventType(tokens)\n",
    "                        articlesData['eventDate'] = feed.publish_date.strftime('%m/%d/%Y')\n",
    "                        articlesData['location'] = getLocation(articleTokens.ents,feed.text)\n",
    "                        \n",
    "                        orgs = set(keyWords).intersection(set(parties))\n",
    "                        if len(orgs) == 0:\n",
    "                            if(articlesData['event']==\"Riots\"):\n",
    "                                taggedOrg = \"Rioters\"\n",
    "                            elif(articlesData['event']==\"Protests\"):\n",
    "                                taggedOrg = \"Protestors\"\n",
    "                            else:\n",
    "                                taggedOrg = \"other\"\n",
    "                        else:\n",
    "                            taggedOrg=\"\"\n",
    "                            for org in orgs:\n",
    "                                taggedOrg+=org+\", \"\n",
    "                            taggedOrg=taggedOrg[:-2]\n",
    "\n",
    "                        articlesData['partiesInvolved'] = taggedOrg\n",
    "                        articlesData['source'] = source\n",
    "                        totalSummary = []\n",
    "                        totalSummary.append(feed.publish_date.strftime(\"On %d %b,\"))\n",
    "                        totalSummary.append(summary)\n",
    "                        extractiveSummary  = (\" \".join(totalSummary))\n",
    "                        articlesData['summary'] = extractiveSummary\n",
    "                        articlesData['link'] = link\n",
    "                        collected['article'].append(articlesData)\n",
    "#                     else:\n",
    "#                         break\n",
    "#                         count +=1\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "        collections['total'].append(collected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rss': 'https://www.einnews.com/rss/T2tHVmwcBIBT0pom',\n",
       "  'link': 'https://www.einnews.com/rss/T2tHVmwcBIBT0pom',\n",
       "  'article': [{'title': \"Friday's best photos: Indonesia protests and VR headsets\",\n",
       "    'event': 'Protests',\n",
       "    'eventDate': '05/10/2019',\n",
       "    'location': 'London',\n",
       "    'partiesInvolved': 'Protestors',\n",
       "    'source': 'World News Report',\n",
       "    'summary': 'On 10 May, London, England Guy Verhofstadt, centre, who leads the Alliance of Liberals and Democrats for Europe and is running for re-election as an MEP, stands with the Liberal Democrat leader, Vince Cable, and party canvassers\\n\\nPhotograph: Matt Dunham/AP',\n",
       "    'link': 'https://www.einnews.com/article/484834140/67AW7Wz6tx7nKhR3?ref=rss&ecode=T2tHVmwcBIBT0pom'},\n",
       "   {'title': 'Indonesia election tainted, opposition marchers say, but government warns against treason - World',\n",
       "    'event': 'Protests',\n",
       "    'eventDate': '05/10/2019',\n",
       "    'location': 'Jakarta',\n",
       "    'partiesInvolved': 'Protestors',\n",
       "    'source': 'World News Report',\n",
       "    'summary': \"On 10 May, The protesters, spurred by unofficial results of last month's election showing that Prabowo Subianto lost to incumbent President Joko Widodo, called for fairness and vigilance in the vote counting process.\\n\\n\",\n",
       "    'link': 'https://www.einnews.com/article/484820064/6yCY5qg2viNG62u5?ref=rss&ecode=T2tHVmwcBIBT0pom'},\n",
       "   {'title': 'Australia’s voters are poised to punish the government',\n",
       "    'event': 'Protests',\n",
       "    'eventDate': '05/11/2019',\n",
       "    'location': 'Islamophobia',\n",
       "    'partiesInvolved': 'Protestors',\n",
       "    'source': 'World News Report',\n",
       "    'summary': 'On 11 May, Yet the Liberals have axed funding for research on it and scrapped initiatives to counter it.',\n",
       "    'link': 'https://www.einnews.com/article/484811977/cNa5YttM44kS5tnN?ref=rss&ecode=T2tHVmwcBIBT0pom'},\n",
       "   {'title': 'ICMI calls on govt to help losing camp overcome disappointment',\n",
       "    'event': 'Protests',\n",
       "    'eventDate': '05/10/2019',\n",
       "    'location': 'Jakarta',\n",
       "    'partiesInvolved': 'Protestors',\n",
       "    'source': 'World News Report',\n",
       "    'summary': \"On 10 May, An exit poll conducted by the Prabowo-Sandi pair camp showed them bagging 55.4 percent of the votes, Coordinator of the pair's National Winning Body's spokespersons Dahnil Anzar Simanjuntak revealed.\\n\\n\\n\\n\",\n",
       "    'link': 'https://www.einnews.com/article/484791521/N3MVFTlAegD5e9D_?ref=rss&ecode=T2tHVmwcBIBT0pom'}]}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections[\"total\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>link</th>\n",
       "      <th>rss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'title': 'Friday's best photos: Indonesia pr...</td>\n",
       "      <td>https://www.einnews.com/rss/T2tHVmwcBIBT0pom</td>\n",
       "      <td>https://www.einnews.com/rss/T2tHVmwcBIBT0pom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  [{'title': 'Friday's best photos: Indonesia pr...   \n",
       "\n",
       "                                           link  \\\n",
       "0  https://www.einnews.com/rss/T2tHVmwcBIBT0pom   \n",
       "\n",
       "                                            rss  \n",
       "0  https://www.einnews.com/rss/T2tHVmwcBIBT0pom  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame = json_normalize(collections['total'])\n",
    "rows = frame.shape[0]\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalDF = pd.DataFrame()\n",
    "for row in range(0,rows):\n",
    "    tempDF = pd.DataFrame()\n",
    "    tempDF = json_normalize(collections['total'][row]['article'])\n",
    "    totalDF = totalDF.append(tempDF, ignore_index = True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe that contains scraped news which has been classified and summarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event</th>\n",
       "      <th>eventDate</th>\n",
       "      <th>link</th>\n",
       "      <th>location</th>\n",
       "      <th>partiesInvolved</th>\n",
       "      <th>source</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Protests</td>\n",
       "      <td>05/10/2019</td>\n",
       "      <td>https://www.einnews.com/article/484834140/67AW...</td>\n",
       "      <td>London</td>\n",
       "      <td>Protestors</td>\n",
       "      <td>World News Report</td>\n",
       "      <td>On 10 May, London, England Guy Verhofstadt, ce...</td>\n",
       "      <td>Friday's best photos: Indonesia protests and V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Protests</td>\n",
       "      <td>05/10/2019</td>\n",
       "      <td>https://www.einnews.com/article/484820064/6yCY...</td>\n",
       "      <td>Jakarta</td>\n",
       "      <td>Protestors</td>\n",
       "      <td>World News Report</td>\n",
       "      <td>On 10 May, The protesters, spurred by unoffici...</td>\n",
       "      <td>Indonesia election tainted, opposition marcher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Protests</td>\n",
       "      <td>05/11/2019</td>\n",
       "      <td>https://www.einnews.com/article/484811977/cNa5...</td>\n",
       "      <td>Islamophobia</td>\n",
       "      <td>Protestors</td>\n",
       "      <td>World News Report</td>\n",
       "      <td>On 11 May, Yet the Liberals have axed funding ...</td>\n",
       "      <td>Australia’s voters are poised to punish the go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Protests</td>\n",
       "      <td>05/10/2019</td>\n",
       "      <td>https://www.einnews.com/article/484791521/N3MV...</td>\n",
       "      <td>Jakarta</td>\n",
       "      <td>Protestors</td>\n",
       "      <td>World News Report</td>\n",
       "      <td>On 10 May, An exit poll conducted by the Prabo...</td>\n",
       "      <td>ICMI calls on govt to help losing camp overcom...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      event   eventDate                                               link  \\\n",
       "0  Protests  05/10/2019  https://www.einnews.com/article/484834140/67AW...   \n",
       "1  Protests  05/10/2019  https://www.einnews.com/article/484820064/6yCY...   \n",
       "2  Protests  05/11/2019  https://www.einnews.com/article/484811977/cNa5...   \n",
       "3  Protests  05/10/2019  https://www.einnews.com/article/484791521/N3MV...   \n",
       "\n",
       "       location partiesInvolved             source  \\\n",
       "0        London      Protestors  World News Report   \n",
       "1       Jakarta      Protestors  World News Report   \n",
       "2  Islamophobia      Protestors  World News Report   \n",
       "3       Jakarta      Protestors  World News Report   \n",
       "\n",
       "                                             summary  \\\n",
       "0  On 10 May, London, England Guy Verhofstadt, ce...   \n",
       "1  On 10 May, The protesters, spurred by unoffici...   \n",
       "2  On 11 May, Yet the Liberals have axed funding ...   \n",
       "3  On 10 May, An exit poll conducted by the Prabo...   \n",
       "\n",
       "                                               title  \n",
       "0  Friday's best photos: Indonesia protests and V...  \n",
       "1  Indonesia election tainted, opposition marcher...  \n",
       "2  Australia’s voters are poised to punish the go...  \n",
       "3  ICMI calls on govt to help losing camp overcom...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "totalDF.to_excel(\"airData/indonesia/testing_\"+now.strftime(\"%m%d%Y_%H%M%S\")+\".xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
